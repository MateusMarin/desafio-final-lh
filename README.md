# ğŸ“˜ ETL Pipeline com Databricks e Terraform

Este repositÃ³rio contÃ©m um pipeline de ELT desenvolvido para extrair, transformar e carregar dados utilizando Databricks e Terraform. A soluÃ§Ã£o Ã© organizada para manter a infraestrutura separada da lÃ³gica de processamento de dados.

## ğŸ›  ConfiguraÃ§Ã£o do Ambiente

### ğŸ”¹ 1. InstalaÃ§Ã£o e ConfiguraÃ§Ã£o do Terraform

Terraform Ã© utilizado para provisionar os catÃ¡logos e esquemas no Unity Catalog do Databricks.

#### **InstalaÃ§Ã£o**

Certifique-se de ter o Terraform instalado:

```bash
# Para sistemas baseados em Debian/Ubuntu
sudo apt-get update && sudo apt-get install -y terraform

# Para macOS
brew install terraform

# Para Windows
choco install terraform
```

Verifique a instalaÃ§Ã£o com:

```bash
terraform -version
```

#### **ConfiguraÃ§Ã£o**

1. Defina as credenciais do Databricks no arquivo `terraform.tfvars`:
   - `databricks_host`: URL do workspace do Databricks.
   - `databricks_token`: Personal Access Token (PAT) do Databricks.
2. Inicialize o Terraform:
   ```bash
   terraform init
   ```
3. Valide a configuraÃ§Ã£o:
   ```bash
   terraform plan
   ```
4. Aplique as configuraÃ§Ãµes:
   ```bash
   terraform apply -auto-approve
   ```

### ğŸ”¹ 2. ConfiguraÃ§Ã£o do Databricks Bundle

Utilizamos o Databricks Asset Bundles para gerenciar os jobs e notebooks.

#### **InstalaÃ§Ã£o e ConfiguraÃ§Ã£o**

1. Instale o Databricks CLI:
   ```bash
   pip install databricks-cli
   ```
2. Autentique-se:
   ```bash
   databricks configure --host <URL_DO_WORKSPACE>
   ```
3. Configure o bundle no Databricks com:
   ```bash
   databricks bundle deploy
   ```
4. Para executar um job manualmente:
   ```bash
   databricks bundle run <nome_do_job>
   ```

Caso a extensÃ£o oficial do Databricks no VSCode seja utilizada, Ã© necessÃ¡rio:

- Python 3.10+
- Foi usada a versÃ£o databricks-connect==13.3.2, mas aceita â‰¥ 13.0.
- Jupyter configurado para usar a venv ativa

Se nÃ£o for utilizada a extensÃ£o, Ã© possÃ­vel configurar credenciais manualmente com `databricks configure` e definir workspace, cluster, org ID, porta e PAT Token.

## ğŸ“Œ GeraÃ§Ã£o do PAT Token no Databricks

1. Acesse o Databricks.
2. Navegue atÃ© `User Settings > Developer`.
3. Selecione `Access Tokens > Create New Token`.
4. Defina a validade e gere o token.
5. Copie e armazene o token com seguranÃ§a.

## ğŸ“„ ExplicaÃ§Ã£o dos Notebooks

### **ExtraÃ§Ã£o de Dados**

- O `extraction_notebook.ipynb` extrai dados do banco de dados SQL Server via JDBC e armazena no Unity Catalog na camada RAW.
- Utiliza `pyspark` e `pandas` para processar os dados.

### **TransformaÃ§Ã£o de Dados**

- O `transformation_notebook.ipynb` consome os dados da RAW, aplica regras de limpeza e transformaÃ§Ã£o para a camada STG.
- Cria tabelas organizadas, remove valores nulos e gera identificadores globais (GlobalID e UniqueID).
- Uma tabela consolidada (`one_big_table`) Ã© criada com base em `SalesOrderHeader` e `SalesOrderDetail`.

## ğŸ“‚ Estrutura de DiretÃ³rios

```
/
â”œâ”€â”€ databricks_bundle/    # ConfiguraÃ§Ã£o do Databricks Asset Bundles
â”œâ”€â”€ src/                  # CÃ³digos-fonte e notebooks
â”œâ”€â”€ terraform/            # Arquivos do Terraform
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md             # Este arquivo
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
```

## ğŸ›  DependÃªncias (requirements.txt)

O arquivo `requirements.txt` contÃ©m todas as dependÃªncias necessÃ¡rias:

```txt
pyspark==3.5.0
pandas
SQLAlchemy==1.4.54
databricks-cli
databricks-connect==13.3.2
```

Para instalar, execute:

```bash
pip install -r requirements.txt
```

## ğŸ” ConfiguraÃ§Ã£o de Secrets no Databricks

Para armazenar credenciais de forma segura:

1. No Databricks, execute:
   ```bash
   databricks secrets create-scope --scope <nome_scope>
   ```
2. Adicione credenciais:
   ```bash
   databricks secrets put --scope <nome_scope> --key <nome_chave>
   ```
3. No notebook, recupere as credenciais:
   ```python
   dbutils.secrets.get(scope="<nome_scope>", key="<nome_chave>")
   ```

## ğŸš€ ConsideraÃ§Ãµes Finais

- Os acessos aos catÃ¡logos e schemas podem ser gerenciados diretamente no Databricks via `Data Explorer`.
- Os jobs podem ser configurados para rodar em diferentes horÃ¡rios e com diferentes triggers.
- A infraestrutura do Terraform e o Databricks Bundle estÃ£o organizados separadamente para manter a modularidade.

Obrigado pela atenÃ§Ã£o! ğŸ¯