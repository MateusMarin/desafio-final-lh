{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformation notebook\n",
    "\n",
    "\n",
    "This notebook is responsible for the transformation of Sales tables from AdventureWorks database from RAW to STG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas SQLAlchemy==1.4.54 pyspark==3.5.0 setuptools\n",
    "dbutils.library.restartPython()  # Reinicia o kernel para carregar as novas bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportaÃ§Ãµes necessÃ¡rias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat_ws, sha2, coalesce\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Criar SparkSession\n",
    "spark = SparkSession.builder.appName(\"TransformRawToStg\").getOrCreate()\n",
    "\n",
    "# Definir catÃ¡logos de origem (RAW) e destino (STG)\n",
    "catalog_raw = \"mateus_marin_raw\"\n",
    "catalog_stg = \"mateus_marin_stg\"\n",
    "\n",
    "print(\"Spark configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Teste Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Exibir versÃ£o do Spark\n",
    "print(f\"Spark versÃ£o: {spark.version}\")\n",
    "\n",
    "# Criar DataFrame de teste\n",
    "data = [(\"Mateus\", 29), (\"JoÃ£o\", 35), (\"Ana\", 23)]\n",
    "columns = [\"Nome\", \"Idade\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar a sessÃ£o Spark (se ainda nÃ£o estiver ativa)\n",
    "spark = SparkSession.builder.appName(\"ETL_SALES\").getOrCreate()\n",
    "\n",
    "# Obter todas as tabelas dinamicamente do schema RAW.sales\n",
    "raw_tables = [row[\"tableName\"] for row in spark.sql(\"SHOW TABLES IN mateus_marin_raw.sales\").collect()]\n",
    "\n",
    "print(f\"ðŸ” Tabelas descobertas no schema RAW.sales: {raw_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_order(table_name):\n",
    "    \"\"\"\n",
    "    Retorna a ordem das colunas para uma tabela especÃ­fica no RAW.sales.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.read.table(f\"mateus_marin_raw.sales.{table_name}\").columns\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao obter colunas da tabela {table_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento de chaves Ãºnicas por tabela\n",
    "unique_keys = {\n",
    "    \"countryregioncurrency\": [\"CountryRegionCode\", \"CurrencyCode\", \"ModifiedDate\"],\n",
    "    \"creditcard\": [\"CreditCardID\", \"ModifiedDate\"],\n",
    "    \"currency\": [\"CurrencyCode\", \"ModifiedDate\"],\n",
    "    \"currencyrate\": [\"CurrencyRateID\", \"ModifiedDate\"],\n",
    "    \"customer\": [\"CustomerID\", \"ModifiedDate\"],\n",
    "    \"personcreditcard\": [\"BusinessEntityID\", \"CreditCardID\", \"ModifiedDate\"],\n",
    "    \"salesorderdetail\": [\"SalesOrderID\", \"SalesOrderDetailID\", \"ModifiedDate\"],\n",
    "    \"salesorderheader\": [\"SalesOrderID\", \"ModifiedDate\"],\n",
    "    \"salesorderheadersalesreason\": [\"SalesOrderID\", \"SalesReasonID\", \"ModifiedDate\"],\n",
    "    \"salesperson\": [\"BusinessEntityID\", \"ModifiedDate\"],\n",
    "    \"salespersonquotahistory\": [\"BusinessEntityID\", \"QuotaDate\", \"ModifiedDate\"],\n",
    "    \"salesreason\": [\"SalesReasonID\", \"ModifiedDate\"],\n",
    "    \"salestaxrate\": [\"SalesTaxRateID\", \"ModifiedDate\"],\n",
    "    \"salesterritory\": [\"TerritoryID\", \"ModifiedDate\"],\n",
    "    \"salesterritoryhistory\": [\"BusinessEntityID\", \"StartDate\", \"TerritoryID\", \"ModifiedDate\"],\n",
    "    \"shoppingcartitem\": [\"ShoppingCartItemID\", \"ModifiedDate\"],\n",
    "    \"specialoffer\": [\"SpecialOfferID\", \"ModifiedDate\"],\n",
    "    \"specialofferproduct\": [\"SpecialOfferID\", \"ProductID\", \"ModifiedDate\"],\n",
    "    \"store\": [\"BusinessEntityID\", \"ModifiedDate\"]\n",
    "}\n",
    "\n",
    "print(\"âœ… Mapeamento de chaves Ãºnicas configurado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "def transform_table(table_name):\n",
    "    \"\"\"\n",
    "    Transforma uma tabela do RAW.sales para STG.sales, aplicando regras especÃ­ficas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lendo a tabela do RAW\n",
    "        df = spark.read.table(f\"mateus_marin_raw.sales.{table_name}\")\n",
    "\n",
    "        # Criar GlobalID de forma consistente\n",
    "        if \"SalesOrderID\" in df.columns and \"SalesOrderDetailID\" in df.columns:\n",
    "            df = df.withColumn(\"GlobalID\", concat_ws(\"-\", col(\"SalesOrderID\"), col(\"SalesOrderDetailID\")))\n",
    "        elif \"SalesOrderID\" in df.columns and \"CustomerID\" in df.columns:\n",
    "            df = df.withColumn(\"GlobalID\", concat_ws(\"-\", col(\"SalesOrderID\"), col(\"CustomerID\")))\n",
    "        elif \"CustomerID\" in df.columns:\n",
    "            df = df.withColumn(\"GlobalID\", col(\"CustomerID\"))\n",
    "        \n",
    "        # Ordenar as colunas\n",
    "        column_order = get_column_order(table_name)\n",
    "        if column_order:\n",
    "            df = df.select(column_order)\n",
    "\n",
    "        # Salvar na camada STG\n",
    "        df.write.mode(\"overwrite\").saveAsTable(f\"mateus_marin_stg.sales.{table_name}\")\n",
    "        print(f\"âœ… Tabela `{table_name}` transformada e carregada na STG.sales!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao transformar a tabela {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in raw_tables:\n",
    "    transform_table(table)\n",
    "\n",
    "print(\"ðŸ TransformaÃ§Ã£o RAW â†’ STG concluÃ­da com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter todas as tabelas disponÃ­veis no STG\n",
    "stg_tables = [row[\"tableName\"] for row in spark.sql(\"SHOW TABLES IN mateus_marin_stg.sales\").collect()]\n",
    "\n",
    "# Mostrar quais tabelas foram identificadas\n",
    "print(f\"ðŸ” Tabelas disponÃ­veis na STG: {stg_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "def rename_conflicting_columns(df, table_name):\n",
    "    \"\"\"\n",
    "    Renomeia colunas `ModifiedDate` e `rowguid` para evitar conflitos de nome antes do JOIN.\n",
    "    Exemplo: `rowguid` da tabela `customer` serÃ¡ `customer_rowguid`.\n",
    "    \"\"\"\n",
    "    if \"ModifiedDate\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"ModifiedDate\", f\"{table_name}_ModifiedDate\")\n",
    "    \n",
    "    if \"rowguid\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"rowguid\", f\"{table_name}_rowguid\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_table(table_name):\n",
    "    \"\"\"\n",
    "    Carrega a tabela STG e aplica correÃ§Ãµes para minimizar valores NULL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.table(f\"{catalog_stg}.sales.{table_name}\")\n",
    "        df = rename_conflicting_columns(df, table_name)\n",
    "\n",
    "        # Corrigir valores NULL dependendo do tipo da coluna\n",
    "        for col_name in df.columns:\n",
    "            dtype = df.schema[col_name].dataType.simpleString()\n",
    "\n",
    "            if dtype.startswith(\"string\"):\n",
    "                df = df.withColumn(col_name, coalesce(col(col_name), lit(\"N/A\")))\n",
    "\n",
    "            elif dtype.startswith(\"int\") or dtype.startswith(\"bigint\") or dtype.startswith(\"smallint\"):\n",
    "                df = df.withColumn(col_name, coalesce(col(col_name), lit(0)))\n",
    "\n",
    "            elif dtype.startswith(\"decimal\") or dtype.startswith(\"double\") or dtype.startswith(\"float\"):\n",
    "                df = df.withColumn(col_name, coalesce(col(col_name), lit(0.0)))\n",
    "\n",
    "            elif dtype.startswith(\"timestamp\") or dtype.startswith(\"date\"):\n",
    "                df = df.withColumn(col_name, coalesce(col(col_name), lit(\"1970-01-01 00:00:00\").cast(\"timestamp\")))\n",
    "\n",
    "        print(f\"âœ… `{table_name}` carregada e prÃ©-processada com sucesso!\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao carregar `{table_name}`: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erro ao carregar `salesorderheader`: [DATATYPE_MISMATCH.DATA_DIFF_TYPES] Cannot resolve \"coalesce(OrderDate, 0)\" due to data type mismatch: Input to `coalesce` should all be the same type, but it's (\"TIMESTAMP\" or \"INT\"). SQLSTATE: 42K09;\n",
      "'Project [SalesOrderID#363, RevisionNumber#444, coalesce(OrderDate#124, 0) AS OrderDate#525, DueDate#125, ShipDate#126, Status#127, OnlineOrderFlag#128, SalesOrderNumber#129, PurchaseOrderNumber#130, AccountNumber#131, CustomerID#132, SalesPersonID#133, TerritoryID#134, BillToAddressID#135, ShipToAddressID#136, ShipMethodID#137, CreditCardID#138, CreditCardApprovalCode#139, CurrencyRateID#140, SubTotal#141, TaxAmt#142, Freight#143, TotalDue#144, Comment#145, ... 2 more fields]\n",
      "+- Project [SalesOrderID#363, coalesce(cast(RevisionNumber#123 as int), 0) AS RevisionNumber#444, OrderDate#124, DueDate#125, ShipDate#126, Status#127, OnlineOrderFlag#128, SalesOrderNumber#129, PurchaseOrderNumber#130, AccountNumber#131, CustomerID#132, SalesPersonID#133, TerritoryID#134, BillToAddressID#135, ShipToAddressID#136, ShipMethodID#137, CreditCardID#138, CreditCardApprovalCode#139, CurrencyRateID#140, SubTotal#141, TaxAmt#142, Freight#143, TotalDue#144, Comment#145, ... 2 more fields]\n",
      "   +- Project [coalesce(SalesOrderID#122, 0) AS SalesOrderID#363, RevisionNumber#123, OrderDate#124, DueDate#125, ShipDate#126, Status#127, OnlineOrderFlag#128, SalesOrderNumber#129, PurchaseOrderNumber#130, AccountNumber#131, CustomerID#132, SalesPersonID#133, TerritoryID#134, BillToAddressID#135, ShipToAddressID#136, ShipMethodID#137, CreditCardID#138, CreditCardApprovalCode#139, CurrencyRateID#140, SubTotal#141, TaxAmt#142, Freight#143, TotalDue#144, Comment#145, ... 2 more fields]\n",
      "      +- Project [SalesOrderID#122, RevisionNumber#123, OrderDate#124, DueDate#125, ShipDate#126, Status#127, OnlineOrderFlag#128, SalesOrderNumber#129, PurchaseOrderNumber#130, AccountNumber#131, CustomerID#132, SalesPersonID#133, TerritoryID#134, BillToAddressID#135, ShipToAddressID#136, ShipMethodID#137, CreditCardID#138, CreditCardApprovalCode#139, CurrencyRateID...\n",
      "âœ… `salesorderdetail` carregada e prÃ©-processada com sucesso!\n",
      "âš ï¸ Coluna 'SalesOrderID' nÃ£o encontrada ou tabelas vazias, pulando JOIN inicial.\n"
     ]
    }
   ],
   "source": [
    "# Carregar tabelas necessÃ¡rias para a criaÃ§Ã£o da `one_big_table`\n",
    "df_salesorderheader = load_table(\"salesorderheader\")\n",
    "df_salesorderdetail = load_table(\"salesorderdetail\")\n",
    "\n",
    "# Garantir que ambas possuem a chave SalesOrderID antes do JOIN\n",
    "if df_salesorderheader and df_salesorderdetail and \"SalesOrderID\" in df_salesorderheader.columns and \"SalesOrderID\" in df_salesorderdetail.columns:\n",
    "    df_final = df_salesorderdetail.join(df_salesorderheader, [\"SalesOrderID\"], \"left\")\n",
    "    print(\"âœ… JOIN inicial entre SalesOrderDetail e SalesOrderHeader realizado com sucesso!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Coluna 'SalesOrderID' nÃ£o encontrada ou tabelas vazias, pulando JOIN inicial.\")\n",
    "    df_final = df_salesorderdetail if df_salesorderdetail else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de tabelas adicionais que serÃ£o adicionadas Ã  `one_big_table`\n",
    "tables_to_join = [\"customer\", \"salesperson\", \"creditcard\", \"salesterritory\"]\n",
    "\n",
    "if df_final:\n",
    "    for table in tables_to_join:\n",
    "        df_temp = load_table(table)\n",
    "        \n",
    "        if df_temp:\n",
    "            # Identificar chaves comuns para JOIN\n",
    "            common_keys = list(set(df_final.columns) & set(df_temp.columns))\n",
    "            \n",
    "            if common_keys:\n",
    "                print(f\"ðŸ”— Fazendo JOIN com `{table}` usando as chaves {common_keys}\")\n",
    "                df_final = df_final.join(df_temp, common_keys, \"left\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Nenhuma chave comum encontrada para `{table}`, pulando...\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Tabela `{table}` nÃ£o carregada, pulando...\")\n",
    "else:\n",
    "    print(\"âŒ `one_big_table` nÃ£o pode ser criada pois o DataFrame base estÃ¡ vazio!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_final:\n",
    "    # Criar UniqueID\n",
    "    if \"SalesOrderID\" in df_final.columns and \"SalesOrderDetailID\" in df_final.columns:\n",
    "        df_final = df_final.withColumn(\"UniqueID\", concat_ws(\"-\", col(\"SalesOrderID\"), col(\"SalesOrderDetailID\")))\n",
    "    elif \"CustomerID\" in df_final.columns:\n",
    "        df_final = df_final.withColumn(\"UniqueID\", col(\"CustomerID\"))\n",
    "\n",
    "    # Criar GlobalID\n",
    "    if \"SalesOrderID\" in df_final.columns and \"CustomerID\" in df_final.columns:\n",
    "        df_final = df_final.withColumn(\"GlobalID\", concat_ws(\"-\", col(\"SalesOrderID\"), col(\"CustomerID\")))\n",
    "\n",
    "    print(\"âœ… UniqueID e GlobalID criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_final:\n",
    "    # Remover duplicatas antes de salvar\n",
    "    df_final = df_final.dropDuplicates()\n",
    "\n",
    "    # Salvar a tabela final apenas se houver dados\n",
    "    if df_final.count() > 0:\n",
    "        df_final.write.mode(\"overwrite\").saveAsTable(f\"{catalog_stg}.sales.one_big_table\")\n",
    "        print(\"âœ… `one_big_table` corrigida e salva com sucesso!\")\n",
    "    else:\n",
    "        print(\"âŒ `one_big_table` nÃ£o pÃ´de ser criada pois nÃ£o hÃ¡ dados disponÃ­veis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_final:\n",
    "    # Exibir estrutura final\n",
    "    print(\"ðŸ“Œ Estrutura da `one_big_table`:\")\n",
    "    df_final.printSchema()\n",
    "\n",
    "    print(\"ðŸ“Œ Exemplo de dados na `one_big_table`:\")\n",
    "    df_final.show(10, False)\n",
    "\n",
    "    from pyspark.sql.functions import sum\n",
    "\n",
    "    # Contagem de valores nulos por coluna\n",
    "    print(\"ðŸ“Œ Contagem de valores nulos por coluna:\")\n",
    "    null_counts = df_final.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_final.columns])\n",
    "    null_counts.show(truncate=False)\n",
    "\n",
    "    # Contagem de registros sem UniqueID e GlobalID\n",
    "    missing_uniqueid_count = df_final.filter(col(\"UniqueID\").isNull()).count()\n",
    "    missing_globalid_count = df_final.filter(col(\"GlobalID\").isNull()).count()\n",
    "    print(f\"âŒ Registros sem UniqueID: {missing_uniqueid_count}\")\n",
    "    print(f\"âŒ Registros sem GlobalID: {missing_globalid_count}\")\n",
    "\n",
    "    # Exemplo de registros sem UniqueID\n",
    "    print(\"ðŸ“Œ Exemplo de registros sem UniqueID:\")\n",
    "    df_final.filter(col(\"UniqueID\").isNull()).show(10, False)\n",
    "\n",
    "    # Verificar colunas duplicadas\n",
    "    duplicate_cols = [c for c in df_final.columns if df_final.columns.count(c) > 1]\n",
    "    print(f\"ðŸ“Œ Colunas duplicadas (se houver): {duplicate_cols}\")\n",
    "\n",
    "    # Exibir as primeiras linhas para conferÃªncia geral\n",
    "    df_final.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a lista de tabelas no schema sales do catalog STG\n",
    "#tables_query = \"SHOW TABLES IN mateus_marin_stg.sales\"\n",
    "#df_tables = spark.sql(tables_query)\n",
    "#\n",
    "## Criar lista de tabelas\n",
    "#tables_list = [row.tableName for row in df_tables.collect()]\n",
    "#\n",
    "## Loop para dropar cada tabela individualmente\n",
    "#for table in tables_list:\n",
    "#    drop_query = f\"DROP TABLE IF EXISTS mateus_marin_stg.sales.{table}\"\n",
    "#    spark.sql(drop_query)\n",
    "#    print(f\"Tabela {table} removida com sucesso! âœ…\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
